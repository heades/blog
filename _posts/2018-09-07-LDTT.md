---
layout: blog
title: "Linear Dependent Type Theory: A Folklore Confirmation"
date: 2018-09-07
categories:
 - linear logic
 - dependent type theory
---

<div id="divCheckbox" style="display: none;">
$$\newcommand{\cat}[1]{\mathcal{#1}}
\newcommand{\func}[1]{\mathsf{#1}}
\newcommand{\iso}[0]{\mathsf{iso}}
\newcommand{\H}[0]{\func{H}}
\newcommand{\J}[0]{\func{J}}
\newcommand{\catop}[1]{\cat{#1}^{\mathsf{op}}}
\newcommand{\Hom}[3]{\mathsf{Hom}_{\cat{#1}}(#2,#3)}
\newcommand{\limp}[0]{\multimap}
\newcommand{\colimp}[0]{\multimapdotinv}
\newcommand{\dial}[1]{\mathsf{Dial_{#1}}(\mathsf{Sets^{op}})}
\newcommand{\dialSets}[1]{\mathsf{Dial_{#1}}(\mathsf{Sets})}
\newcommand{\dcSets}[1]{\mathsf{DC_{#1}}(\mathsf{Sets})}
\newcommand{\sets}[0]{\mathsf{Sets}}
\newcommand{\obj}[1]{\mathsf{Obj}(#1)}
\newcommand{\mor}[1]{\mathsf{Mor(#1)}}
\newcommand{\id}[0]{\mathsf{id}}
\newcommand{\lett}[0]{\mathsf{let}\,}
\newcommand{\inn}[0]{\,\mathsf{in}\,}
\newcommand{\cur}[1]{\mathsf{cur}(#1)}
\newcommand{\curi}[1]{\mathsf{cur}^{-1}(#1)}
\newcommand{\mto}[0]{\to}
\newcommand{\m}[0]{\mathsf{m}}
\newcommand{\w}[0]{\mathsf{w}}
\newcommand{\c}[0]{\mathsf{c}}
\newcommand{\Type}[0]{\mathsf{Type}}
$$
</div>

Almost exactly one year ago [Dominic Orchard](https://www.cs.kent.ac.uk/people/staff/dao7/) and I have joined forces to make progress on new functional programming languages firmly founded in substructural type systems.  We are calling our project the [Granule Project](NEED A DOMAIN NAME).  I am not going to introduce the Granule Project, because Dominic has done a great job in his [post]().  In fact, this post is part of a series describing all of our work.  For the list of all the posts in the series please see this [table of contents]().  We are using this series to spread the word about our work pre-publication.

My role in the project is to study the combination of graded modal types and dependent type theory.  This combination allows us to solve a long standing open question of how to define substructural dependent type theory.  However, before I can move on to explaining these new goodies like graded modal types I first need to argue why defining substructural dependent type theories is hard.  This is exactly what this post attempts to do.

In this post we will stick to linear logic, and discuss what we call *Linear Dependent Type Theory (LDTT)*, but what do we mean by this?  We define LDTT to be a dependent type theory where *every* variable -- including type variables -- must be used exactly once.  So for example, if one has a closed lambda-expression

$$\emptyset \vdash \lambda x.t : (x : A) \limp B$$

then $$x$$ must be used exactly once in $$t$$ and exactly once in $$B$$.  If we have an open expression like

$$\Gamma_1,x : A,\Gamma_2 \vdash t : B$$

then $$x$$ must appear only once in $$\Gamma_2$$, or appear only once in $$t$$, or appear only once in $$B$$, but it must appear in one of those.  There are some interesting points to be discussed about this definition, but let's delay that discussion, and use this informal definition to figure out just how useful this language is from a programmatic perspective.

Let's start simple, and consider the polymorphic identity function:

$$\emptyset \vdash \lambda A.\lambda x.x : (A : \Type) \limp (x : A) \limp A$$

Well this is discouraging, because notice that this definition violates linearity!  The variable $$A$$ in the term is never used, the variable $$A$$ is used twice in the type, and the variable $$x$$ in the type is never used.

In fact, if we think about it, forcing every variable to be used exactly once in the type means that *every* $$B$$ of a type like $$(x_1 : A_1) \limp \cdots \limp (x_i : A_i) \limp B$$, where $$B$$ is not a dependent arrow type or a dependent tensor product, must be dependent.  Thus, to even get started this language *must* have some notion of atomic dependent type that allows us to use our final input variable $$x_i$$.  But, it's even worse.

One property of dependent type theory that is extremely important is the ability to define the simply-typed arrow, $$(A \limp B) := ((x : A) \limp B)$$ where $$x \not\in FV(B)$$.  However, as we can see, this definition requires $$x$$ to be used zero times in $$B$$, but linearity says this is not allowed.

At this point we can see that LDTT is not looking very useful.  Even basic programs do not have types in LDTT.  Now whenever I tell folks about this they seem generally surprised, but convinced, because it makes a lot of sense when you start thinking about it, but it is not super wide known.  However, most researchers studying substructural type theory do know about these problems, but no one has written down a formal proof that a system like LDTT is not very useful.  Thus, it is a folklore result; if you want to know what I think about folklore results see this [post](https://blog.metatheorem.org/2018/07/24/Linear-Categories-A-Folklore-Simplification.html), but in summary, I don't like them.

In this post I make this formal, and prove a trivialization theorem showing that LDTT is trivial; more precisely, I show that the only closed typable programs in LDTT are programs with atomic type.  Before we can do all this we need to pin down the formal definition of LDTT.  Let's start with that.

The syntax of LDTT expressions is described by the following grammar:

<img width="90%" src="/blog/images/posts/2018-09-08-LDTT/LDTT-Syntax.png">

We can see that we are using a collapsed syntax, and so the distinction between types and programs will be made judgmentally.  The syntax is not too surprising, but we do denote linear tensoring of programs by $$(t_1,t_2)$$ instead of the typical $$t_1 \otimes t_2$$, but the reason for this will become apparent in my later post on adding graded modalities.  For now, just think of these as pairs that have no projections.

The typing judgment $$\Gamma \vdash t : A$$ is defined by the following set of inference rules:

<img width="95%" src="/blog/images/posts/2018-09-08-LDTT/LDTT-typing.png">

Anyone familiar with dependent type theory will not find the existence of each of the above rules surprising.  We have both kinding rules, $$\Type$$, $$\limp$$, $$\otimes$$, and introduction and elimination rules, $$\mathsf{Var}$$, $$\limp_i$$, $$\limp_e$$, $$\otimes_i$$, and $$\otimes_e$$.  However, there are some aspects of these rules that the reader might find interesting.

First, the rules are designed in the same way we design linear logics.  Any two contexts $$\Gamma_i$$ and $$\Gamma_j$$ are assumed to be disjoint when $$i \neq j$$.  This constraint prevents contraction from being admissible.  However, there is one rule that is different from how we define the same rule in linear logic:

<center><img width="50%" src="/blog/images/posts/2018-09-08-LDTT/LDTT-var.png"></center>

In linear logic, this rule usually does not allow for $$\Gamma$$ to be present, but in dependent type theory, the type $$A$$ may depend on other variables, thus, we insure that this type is well-formed by requiring that $$\Gamma \vdash A : \Type_l$$ holds.  Furthermore, there might be dependencies between types and variables within $$\Gamma$$, and so, we must enforce that if a variable in $$\Gamma$$ is used by some other type in $$\Gamma$$ then it is used at most once, and this is enforced by the premise $$\dashv (\Gamma,x : A)$$.  In fact, if we remove that premise from the rule we can type check non-linear programs.

A close inspection of each rule while keeping linearity in mind will result in one considering a few rules to be dubious. For example, consider the following rule:

<center><img width="65%" src="/blog/images/posts/2018-09-08-LDTT/LDTT-limp.png"></center>

The context $$\Gamma_1$$ is used to check that $$A$$ is well typed, but it also appears in the second premise.  This looks an awful lot like a duplication.  However, we can prove it is not allowing any form of duplication.  First, $$\Gamma_1$$ must appear in the second premise, because $$B$$ depends on $$x$$ whose type is $$A$$, and so, $$A$$'s dependencies must be in scope.  Second, as long as LDTT enforces linearity properly, then every variable in $$\Gamma_1$$ is either used exactly once in $$\Gamma_1$$ or exactly once in $$A$$, and this implies that in the second premise every element of $$\Gamma_1$$ is already used linearly, and thus, we just have to be sure $$\Gamma_2$$ nor $$B$$ use any of the variables in $$\Gamma_1$$.

We can show that if $$\Gamma \vdash t : A$$, then $$\Gamma$$ is well typed and ordered based on dependency, furthermore, if a variable in $$\Gamma$$ is used in $$\Gamma$$, then it is used exactly once, that is, $$\dashv \Gamma$$ holds.  Thus, in the second premise above, we know that $$\Gamma_1$$ is not depended upon by $$\Gamma_2$$.  As for whether $$B$$ depends upon $$\Gamma_1$$, define $$t \rtimes \Gamma$$ to hold iff the set of free variables of $$t$$ is disjoint from the set of first projections of $$\Gamma_1$$.  We call $$t \rtimes \Gamma$$ *disjointness*.  We can prove the following disjointness lemma:

<span class="lemma"> If $$\Gamma \vdash t : B$$, $$x : A \in \Gamma$$, $$\Gamma_1 \subseteq \Gamma$$, and $$\Gamma_1 \vdash A : \Type_l$$, then $$t \rtimes \Gamma_1$$.</span>

Now applying this to the premises of the $$\limp$$-rule we can see that we know $$B \rtimes \Gamma_1$$.   Therefore, even though it looks as if a duplication is happening in rules like the $$\limp$$-rule, well-formedness of contexts and disjointness confirms that no duplication is occurring.

These results are enough to prove the linearity theorem.

<span class="theorem">If $$\Gamma \vdash t : B$$, then for every $$x : A \in \Gamma$$, $$x$$ appears only once in $$\Gamma$$, or only once in $$t$$, or only once in $$B$$.</span>

This result confirms we have a well-defined system, but it turns out we can now prove that this system is trivial.  First, we can show the following.

<span class="lemma">If $$\emptyset \vdash A : \Type_{l'}$$, then $$A$$ is $$\Type_l$$ for some level $$l$$.</span>

Using this result a long with the following result:

<span class="lemma">If $$\Gamma \vdash t : A$$, $$x : A \in \Gamma$$, then there is a $$\Gamma_1 \subseteq \Gamma$$, such that, $$\Gamma_1 \vdash A : \Type_l$$ for some level $$l$$.</span>

we obtain the following trivialization theorem:

<span class="theorem">If $$\emptyset \vdash t : A$$, then $$t$$ is $$\Type_{l_1}$$ and $$A$$ is $$\Type_{l_2}$$ for some $$l_1$$ and $$l_2$$.</span>

This result explains a lot about the current landscape of papers on the topic of dependent type theory for linear logic like [Luo and Zhang's paper](http://www.cs.rhul.ac.uk/~zhaohui/TYPES16.pdf), [McBride's paper](https://personal.cis.strath.ac.uk/conor.mcbride/pub/Rig.pdf), [Atkey's paper](https://bentnib.org/quantitative-type-theory.html), and [Abel's paper](http://www.cse.chalmers.se/~abela/types18.pdf).  They all make a design decision about how variables should by used by relaxing linearity in one way or another.  At first, I was very confused, because no one mentions what goes wrong with the naive approach.  We have confirmed in this post that there is a problem, and we are forced to relax linearity in some way.  In the next post, I will show how we extend LDTT to allow the programmer to decide how variables are used instead of enforcing an arbitrary design decision on them.